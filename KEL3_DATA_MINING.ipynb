{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fauzinurfalah/SentimentIMDB/blob/main/KEL3_DATA_MINING.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O57U4Q_6jlsL"
      },
      "source": [
        "Kelompok 3 Data Mining\n",
        "1. Ahda Faizalziddi Al Aziz     (STI202303600)\n",
        "2. Ilham Aufa Nugroho           (STI202303346)\n",
        "3. Fauzi Nur Falah              (STI202303409)\n",
        "4. Muhammad Syarif Hidayatullah (STI202303527)\n",
        "5. Tri Wahyu Hidayat            (STI202303353)\n",
        "6. Muhammad Roy Setiawan        (STI202303331)\n",
        "7. Sahrur Anam                  (STI202303552)\n",
        "8. Muh Januar Imam Saputra      (STI202202623)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Semt-pwRkiez",
        "outputId": "674af011-ee8f-47f7-dc9b-d976abde0d38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.12/dist-packages (1.9.4)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.12/dist-packages (0.14.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# Instal library\n",
        "!pip install pandas numpy scikit-learn seaborn matplotlib nltk wordcloud imbalanced-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "95YYMwl-kxQS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65ed346e-8d35-45a6-b645-4e27f6888549"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# ML & NLP\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
        "\n",
        "# Resampling\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "# NLTK stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "jsHPBEbU_HDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGfTY4_V9KnC"
      },
      "outputs": [],
      "source": [
        "path = '/content/drive/IMDB_Dataset.csv'\n",
        "df = pd.read_csv('IMDB_Dataset.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oDsn5qRoKkO"
      },
      "outputs": [],
      "source": [
        "# Hapus nilai kosong\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'<.*?>', '', text)          # hapus HTML tag\n",
        "    text = re.sub(r'[^a-zA-Z]', ' ', text)     # hapus angka & simbol\n",
        "    text = text.lower()                        # ubah ke huruf kecil\n",
        "    text = text.split()                        # tokenisasi sederhana\n",
        "    text = [word for word in text if word not in stop_words]  # hapus stopwords\n",
        "    return ' '.join(text)\n",
        "\n",
        "df.dropna(inplace=True)\n",
        "df['clean_review'] = df['review'].apply(clean_text)\n",
        "df['label'] = df['sentiment'].map({'negative':0, 'positive':1})\n",
        "df = df[['clean_review', 'label']].dropna()\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EN6g5b7QoUJf"
      },
      "outputs": [],
      "source": [
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['clean_review'], df['label'],\n",
        "    test_size=0.2, random_state=42, stratify=df['label']\n",
        ")\n",
        "\n",
        "# Vectorizer & to_dense untuk SMOTE\n",
        "tfidf = TfidfVectorizer(max_features=5000, min_df=2)\n",
        "to_array = FunctionTransformer(lambda x: x.toarray(), accept_sparse=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pipeline SMOTE untuk 2 model\n",
        "pipelines = {\n",
        "    \"NaiveBayes_SMOTE\": ImbPipeline(steps=[\n",
        "        (\"tfidf\", tfidf),\n",
        "        (\"to_array\", to_array),\n",
        "        (\"smote\", SMOTE(random_state=42, k_neighbors=5)),\n",
        "        (\"clf\", MultinomialNB())\n",
        "    ]),\n",
        "    \"LogReg_SMOTE\": ImbPipeline(steps=[\n",
        "        (\"tfidf\", tfidf),\n",
        "        (\"to_array\", to_array),\n",
        "        (\"smote\", SMOTE(random_state=42, k_neighbors=5)),\n",
        "        (\"clf\", LogisticRegression(max_iter=400))\n",
        "    ])\n",
        "}"
      ],
      "metadata": {
        "id": "mmuVpSgpkDYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcIUBlo6ob0o"
      },
      "outputs": [],
      "source": [
        "# Training, Prediksi, Evaluasi\n",
        "rows = []\n",
        "cms = {}\n",
        "\n",
        "for name, pipe in pipelines.items():\n",
        "    pipe.fit(X_train, y_train)\n",
        "    pred = pipe.predict(X_test)\n",
        "\n",
        "    acc = accuracy_score(y_test, pred)\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
        "        y_test, pred, average='binary', pos_label=1, zero_division=0\n",
        "    )\n",
        "    cm = confusion_matrix(y_test, pred)\n",
        "\n",
        "    rows.append([name, acc, prec, rec, f1])\n",
        "    cms[name] = cm\n",
        "\n",
        "results = pd.DataFrame(rows, columns=[\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\"]).sort_values(\"F1\", ascending=False)\n",
        "print(results.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73axBDDeoe01"
      },
      "outputs": [],
      "source": [
        "# Classification Report detail\n",
        "for name, pipe in pipelines.items():\n",
        "    pred = pipe.predict(X_test)\n",
        "    print(f\"\\n=== Classification Report: {name} ===\")\n",
        "    print(classification_report(y_test, pred, digits=4))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Confusion Matrix untuk Kedua Model\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10,4))\n",
        "for ax, (name, cm) in zip(axes, cms.items()):\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
        "    ax.set_title(f\"Confusion Matrix - {name}\")\n",
        "    ax.set_xlabel(\"Prediksi\")\n",
        "    ax.set_ylabel(\"Aktual\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "t7i3BPEgkepM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best = results.iloc[0]\n",
        "print(f\"Model terbaik (berdasarkan F1): {best['Model']} | Acc={best['Accuracy']:.4f}, Prec={best['Precision']:.4f}, Rec={best['Recall']:.4f}, F1={best['F1']:.4f}\")"
      ],
      "metadata": {
        "id": "Td3shQxgknku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTMxX2-lohsV"
      },
      "outputs": [],
      "source": [
        "wc_kwargs = dict(\n",
        "    width=1200,\n",
        "    height=600,\n",
        "    background_color=\"white\",\n",
        "    stopwords=stop_words,\n",
        "    collocations=False\n",
        ")\n",
        "\n",
        "# Ambil model terbaik dan prediksi\n",
        "best_name = results.sort_values(\"F1\", ascending=False).iloc[0][\"Model\"]\n",
        "best_pipe = pipelines[best_name]\n",
        "y_pred = best_pipe.predict(X_test)\n",
        "\n",
        "# Buat DataFrame untuk analisis prediksi\n",
        "test_df = pd.DataFrame({\n",
        "    \"text\": X_test.values,\n",
        "    \"y_true\": y_test.values,\n",
        "    \"y_pred\": y_pred\n",
        "})\n",
        "\n",
        "# Gabungkan teks berdasarkan hasil prediksi\n",
        "pos_pred_text = \" \".join(test_df.loc[test_df[\"y_pred\"] == 1, \"text\"].astype(str).values)\n",
        "neg_pred_text = \" \".join(test_df.loc[test_df[\"y_pred\"] == 0, \"text\"].astype(str).values)\n",
        "\n",
        "# Tampilkan WordCloud\n",
        "plt.figure(figsize=(14,6))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(WordCloud(**wc_kwargs).generate(pos_pred_text))\n",
        "plt.title(f\"WordCloud — Prediksi Positif ({best_name})\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(WordCloud(**wc_kwargs).generate(neg_pred_text))\n",
        "plt.title(f\"WordCloud — Prediksi Negatif ({best_name})\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}